# -*- coding: utf-8 -*-
"""DBCV_paper_real.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1raURMYdOP2hCxqMjKUHk3PIJMkPPYV_e

Usare rpy2

Aggiungere https://github.com/pajaskowiak/clusterConfusion e rimuovere CVDD

https://github.com/davidechicco/DBCVindex

aggiornare repository con nuovo codice

#**Library**
"""

#%load_ext rpy2.ipython

# %%R
# install.packages("devtools")
# install.packages("cluster")
# install.packages("pegas")
# install.packages("proxy")

# %%R
# system("git clone https://github.com/janagauss/dcsi.git")
# source("dcsi/code/functions/separability_functions.R")

# pip install "git+https://github.com/FelSiq/DBCV"

pip install hdbscan

# pip install --upgrade cdbw

# !git clone https://github.com/senolali/VIASCKDE.git
# da citare:
#"Ali Şenol, "VIASCKDE Index: A Novel Internal Cluster Validity Index for Arbitrary-Shaped Clusters Based on the Kernel Density Estimation",
# Computational Intelligence and Neuroscience, vol. 2022, Article ID 4059302, 20 pages, 2022. https://doi.org/10.1155/2022/4059302"

# !wget https://raw.githubusercontent.com/senolali/VIASCKDE/master/VIASCKDE.py

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons, make_circles
import hdbscan
from hdbscan import HDBSCAN
import time
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import euclidean
#from dbcv import dbcv
import random
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import silhouette_score, adjusted_rand_score, adjusted_mutual_info_score,adjusted_rand_score , davies_bouldin_score, calinski_harabasz_score
from sklearn.cluster import DBSCAN
from sklearn.utils import resample
from scipy.stats import entropy
import math
from scipy.spatial.distance import pdist, squareform
from sklearn.cluster import KMeans
from sklearn.utils import resample
import scipy.io
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import MeanShift, estimate_bandwidth
from scipy.special import erf
import numpy as np
import warnings
from matplotlib.backends.backend_pdf import PdfPages
# from cdbw import CDbw
# from threading import Thread
# from queue import Queue
# from sklearn.exceptions import NotFittedError
# import VIASCKDE
#warnings.simplefilter(action='ignore', category=FutureWarning)

# from sklearn.metrics import pairwise_distances
# from rpy2.robjects import numpy2ri
# import rpy2.robjects as robjects
# from rpy2.robjects import r
# numpy2ri.activate()

from google.colab import output

!wget https://ssd.mathworks.com/supportfiles/downloads/R2025a/Release/0/deployment_files/installer/complete/glnxa64/MATLAB_Runtime_R2025a_glnxa64.zip
!unzip MATLAB_Runtime_R2025a_glnxa64.zip -d MATLAB_Runtime
!sudo ./MATLAB_Runtime/install -agreeToLicense yes
output.clear()



"""#**Function**"""

def optimalK(data, maxClusters):
    """
    Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie
    Params:
        data: ndarray of shape (n_samples, n_features)
        maxClusters: Maximum number of clusters to test for
    Returns: (gaps, optimalK)
    """
    nrefs = 3
    gaps = np.zeros((len(range(1, maxClusters)),))
    results = []

    for gap_index, k in enumerate(range(1, maxClusters)):

        refDisps = np.zeros(nrefs)

        for i in range(nrefs):
            randomReference = np.random.random_sample(size=data.shape)
            km = KMeans(n_clusters=k)
            km.fit(randomReference)

            refDisp = km.inertia_
            refDisps[i] = refDisp
        km = KMeans(n_clusters=k)
        km.fit(data)

        origDisp = km.inertia_

        gap = np.log(np.mean(refDisps)) - np.log(origDisp)

        gaps[gap_index] = gap
        results.append({'clusterCount': k, 'gap': gap})
    resultsdf = pd.DataFrame(results)
    return gaps.argmax() + 1, resultsdf

def generate_moon_datasets(seed):
    datasets = []

    for noise in [0.0, 0.056, 0.111, 0.167, 0.222, 0.278, 0.333, 0.389, 0.444, 0.5]:
        X, _ = make_moons(n_samples=1000, noise=noise, random_state=seed)
        datasets.append(X)

    return datasets

def plot_datasets(datasets):
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))

    for i, dataset in enumerate(datasets):
        row = i // 5
        col = i % 5
        ax = axes[row, col]
        ax.scatter(dataset[:, 0], dataset[:, 1], c='black', s=1)
        ax.set_title(f'Dataset {i+1}')
        ax.set_xlim(-1.5, 2.5)
        ax.set_ylim(-1.0, 1.5)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def plot_best_results_compare(datasets, results, best_params):
    fig, axs = plt.subplots(2, 5, figsize=(20, 8))
    min_samples, epsilon = best_params

    for i, (X, dataset_results) in enumerate(zip(datasets, results)):
        print(dataset_results)
        for min_samples_, epsilon_, dbcv_score, silhouette, dunn_index, davies_bouldin, calinski_harabasz,shannon_entropy, labels in dataset_results:
            if (min_samples_, epsilon_) == best_params:
                ax = axs[i // 5, i % 5]
                ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=10)
                ax.set_title(f'min_samples={min_samples}, epsilon={epsilon}\ndbcv={dbcv_score:.3f}\nsilhouette={silhouette:.3f}\ndunn={dunn_index:.3f}\ndavies={davies_bouldin:.3f}\ncalinski={calinski_harabasz:.3f}\nshannon_entropy={shannon_entropy:.3f}')
                ax.set_xticks([])
                ax.set_yticks([])
                break

    plt.grid(True)
    plt.tight_layout()
    plt.show()

#Se non funziona usare funzione precedente
def calculate_dunn_index(X, labels):

    X = np.array(X) if not isinstance(X, np.ndarray) else X
    unique_labels = np.unique(labels)
    if len(unique_labels) < 2:
        return -1

    distances = np.sqrt(((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2).sum(axis=2))
    min_intercluster_distances = np.inf
    for label_i in unique_labels:
        cluster_i = X[labels == label_i]
        for label_j in unique_labels:
            if label_i != label_j:
                cluster_j = X[labels == label_j]
                intercluster_distances = np.min(distances[np.ix_(labels == label_i, labels == label_j)])
                min_intercluster_distances = min(min_intercluster_distances, intercluster_distances)
    max_intracluster_distances = 0
    for label in unique_labels:
        cluster = X[labels == label]
        intracluster_distances = np.max(distances[np.ix_(labels == label, labels == label)])
        max_intracluster_distances = max(max_intracluster_distances, intracluster_distances)
    return min_intercluster_distances / max_intracluster_distances

def analyze_datasets(datasets):
    results = []
    param_grid = {
        'min_samples': [10],
        'epsilon': [0.1]
    }

    for X in datasets:
        dataset_results = []
        for min_samples in param_grid['min_samples']:
            for epsilon in param_grid['epsilon']:
                clusterer = DBSCAN(min_samples=min_samples, eps=epsilon)
                #clusterer = hdbscan.HDBSCAN(min_samples = min_samples, cluster_selection_epsilon=epsilon)
                labels = clusterer.fit_predict(X)

                dbcv_score = dbcv(X, labels)
                silhouette = silhouette_score(X, labels) if len(set(labels)) > 1 else -1
                dunn_index = calculate_dunn_index(X, labels)
                davies_bouldin = davies_bouldin_score(X, labels) if len(set(labels)) > 1 else -1
                calinski_harabasz = calinski_harabasz_score(X, labels) if len(set(labels)) > 1 else -1
                shannon_entropy = calculate_shannon_entropy(labels)
                dataset_results.append((min_samples, epsilon, dbcv_score, silhouette, dunn_index,
                                        davies_bouldin, calinski_harabasz, shannon_entropy, labels))

        results.append(dataset_results)

    return results

def analyze_datasets_best_params(datasets):
    results = []
    param_grid = {
        'min_samples': [1, 5, 10, 50],
        'epsilon': [0.01, 0.1, 0.2, 0.5]
    }

    for X in datasets:
        dataset_results = []

        for min_samples in param_grid['min_samples']:
            for epsilon in param_grid['epsilon']:
                clusterer = DBSCAN(min_samples=min_samples, eps=epsilon)
                labels = clusterer.fit_predict(X)
                dbcv_score = dbcv(X, labels)
                dataset_results.append((min_samples, epsilon, dbcv_score,labels))

        results.append(dataset_results)

    return results

def find_best_params(results):
    param_dbvcs = {}

    for dataset_results in results:
        for min_samples, epsilon, dbcv_score, labels in dataset_results:
            param_key = (min_samples, epsilon)
            if param_key not in param_dbvcs:
                param_dbvcs[param_key] = []
            param_dbvcs[param_key].append(dbcv_score)

    param_avg_dbcv = {k: np.mean(v) for k, v in param_dbvcs.items()}

    best_params = max(param_avg_dbcv, key=param_avg_dbcv.get)

    return best_params

import matplotlib.pyplot as plt

def plot_best_results(datasets, results, best_params):
    fig, axs = plt.subplots(2, 5, figsize=(20, 8))
    min_samples, epsilon = best_params
    min_samples = 2
    epsilon = 0.1

    for i, (X, dataset_results) in enumerate(zip(datasets, results)):
        for min_samples_, epsilon_, dbcv_score, labels in dataset_results:
            if (min_samples_, epsilon_) == best_params:
                ax = axs[i // 5, i % 5]
                ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=10)
                if dbcv_score is not None:
                    ax.set_title(f'min_samples={min_samples}, epsilon={epsilon}\ndbcv={dbcv_score:.3f}')
                else:
                    ax.set_title(f'min_samples={min_samples}, epsilon={epsilon}\ndbcv=N/A')
                ax.set_xticks([])
                ax.set_yticks([])
                break

    plt.grid(True)
    plt.tight_layout()
    plt.show()

def calculate_shannon_entropy(labels):
    labels = np.array(labels) if not isinstance(labels, np.ndarray) else labels
    unique, counts = np.unique(labels, return_counts=True)
    probabilities = counts / len(labels)
    entropy = -np.sum(probabilities * np.log2(probabilities))
    # labels = labels + 1
    # entropy_val = entropy(np.bincount(labels) / len(labels), base=2)
    # entropy_val = abs(1-entropy_val)
    return entropy

def plot_datasets_with_clustering(datasets):
    fig, axes = plt.subplots(5, 3, figsize=(18, 15))
    dbcv_total = []
    cvdd_dbcv_total = []
    chi_dbcv_total = []
    du_dbcv_total = []
    db_dbcv_total = []

    pdf_path1 = "clustering_plots_part1.pdf"
    pdf_path2 = "clustering_plots_part2.pdf"

    with PdfPages(pdf_path1) as pdf1, PdfPages(pdf_path2) as pdf2:
        for i, dataset in enumerate(datasets):
            if i == 5:
                plt.tight_layout()
                pdf1.savefig(fig)
                plt.close(fig)
                fig, axes = plt.subplots(5, 3, figsize=(18, 15))

            X = dataset
            ax_left = axes[i % 5, 0]
            ax_left.scatter(X[:, 0], X[:, 1], c='black', s=10)
            ax_left.set_title(f'Dataset {i+1} - Original')
            ax_left.set_xlim(-1.5, 2.5)
            ax_left.set_ylim(-1.0, 1.5)
            ax_left.grid(True, color='lightgrey', linewidth=0.5)

            ax_middle = axes[i % 5, 1]
            clusterer = DBSCAN(min_samples=10, eps=0.1)
            labels = clusterer.fit_predict(X)

            scatter = ax_middle.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=5)
            ax_middle.set_title(f'Dataset {i+1} - DBSCAN')
            ax_middle.set_xlim(-1.5, 2.5)
            ax_middle.set_ylim(-1.0, 1.5)
            ax_middle.grid(True, color='lightgrey', linewidth=0.5)

            dbcv_score = dbcvapi.s_dbcv(X, labels)
            dbcv_total.append(dbcv_score)

            db_dbcv = dbcvapi.db_dbcv(X, labels)
            db_dbcv_total.append(db_dbcv)

            du_dbcv = dbcvapi.di_dbcv(X, labels)
            du_dbcv_total.append(du_dbcv)

            chi_dbcv = dbcvapi.chi_dbcv(X, labels)
            chi_dbcv_total.append(chi_dbcv)

            cvdd_dbcv = dbcvapi.CVDDIndex(X, labels)
            cvdd_dbcv_total.append(cvdd_dbcv)


            num_clusters = len(set(labels))
            ax_right = axes[i % 5, 2]
            ax_right.axis('off')

            metrics_text = (f'DBCV: {round(float(dbcv_score), 4)}\n'
                            f'Density-Based Davies-Bouldin: {round(float(db_dbcv), 4)}\n'
                            f'Density-Based Dunn index: {round(float(du_dbcv), 4)}\n'
                            f'Density-Based Calinski-Harabasz: {round(float(chi_dbcv), 4)}\n'
                            f'Density-Based CVDD: {round(float(cvdd_dbcv), 4)}\n'
                            f'Number of clusters: {len(set(labels))}')

            ax_right.text(0.1, 0.5, metrics_text, fontsize=14, verticalalignment='center',
              bbox=dict(facecolor='white', alpha=0.8, edgecolor='black', linewidth=1.5, pad=5))

        plt.tight_layout()
        pdf2.savefig(fig)
        plt.close(fig)

    return dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total

def generate_circle_data(n_points, radius, shift):
    t = np.linspace(0, 2 * np.pi, n_points)
    x = radius * np.cos(t) + shift[0]
    y = radius * np.sin(t) + shift[1]
    return np.column_stack((x, y))

def generate_datasets_with_two_circles(n_datasets, n_points_per_circle, n_points_per_circle_inner, inner_radius, outer_radius, initial_shift, shift_increment):
    datasets = []
    for i in range(n_datasets):
        outer_circle = generate_circle_data(n_points_per_circle, outer_radius, (0, 0))

        shift = (initial_shift[0] + i * shift_increment[0], initial_shift[1] + i * shift_increment[1])
        inner_circle = generate_circle_data(n_points_per_circle_inner, inner_radius, shift)

        dataset = np.concatenate([outer_circle, inner_circle])
        datasets.append(dataset)
    return datasets

def plot_datasets_with_clustering_circle(datasets):
    fig, axes = plt.subplots(5, 3, figsize=(30, 25))
    dbcv_total = []
    cvdd_dbcv_total = []
    chi_dbcv_total = []
    du_dbcv_total = []
    db_dbcv_total = []


    pdf_path1 = "clustering_plots_part1_shifting_circles.pdf"
    pdf_path2 = "clustering_plots_part2_shifting_circles.pdf"

    with PdfPages(pdf_path1) as pdf1, PdfPages(pdf_path2) as pdf2:
        for i, dataset in enumerate(datasets):
            if i == 5:
                plt.tight_layout()
                pdf1.savefig(fig)
                plt.close(fig)
                fig, axes = plt.subplots(5, 3, figsize=(30, 25))

            X = np.unique(dataset, axis=0)
            x_min, x_max = -2, 2.5#X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
            y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

            ax_left = axes[i % 5, 0]
            ax_left.scatter(X[:, 0], X[:, 1], c='black', s=10)
            ax_left.set_title(f'Dataset {i+1} - Original')
            ax_left.set_xlim(x_min, x_max)
            ax_left.set_ylim(y_min, y_max)
            ax_left.set_aspect('equal')
            ax_left.grid(True, color='lightgrey', linewidth=0.5)

            ax_middle = axes[i % 5, 1]
            clusterer = DBSCAN(eps=0.1, min_samples=20)
            labels = clusterer.fit_predict(X)

            scatter = ax_middle.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=5)
            ax_middle.set_title(f'Dataset {i+1} - DBSCAN')
            ax_middle.set_xlim(x_min, x_max)
            ax_middle.set_ylim(y_min, y_max)
            ax_middle.set_aspect('equal')
            ax_middle.grid(True, color='lightgrey', linewidth=0.5)

            dbcv_score = dbcvapi.s_dbcv(X, labels)
            dbcv_total.append(dbcv_score)

            db_dbcv = dbcvapi.db_dbcv(X, labels)
            db_dbcv_total.append(db_dbcv)

            du_dbcv = dbcvapi.di_dbcv(X, labels)
            du_dbcv_total.append(du_dbcv)

            chi_dbcv = dbcvapi.chi_dbcv(X, labels)
            chi_dbcv_total.append(chi_dbcv)

            cvdd_dbcv = dbcvapi.CVDDIndex(X, labels)
            cvdd_dbcv_total.append(cvdd_dbcv)




            num_clusters = len(set(labels)) - (1 if -1 in labels else 0)

            ax_right = axes[i % 5, 2]
            ax_right.axis('off')

            metrics_text = (f'DBCV: {round(float(dbcv_score), 4)}\n'
                            f'Density-Based Davies-Bouldin: {round(float(db_dbcv), 4)}\n'
                            f'Density-Based Dunn index: {round(float(du_dbcv), 4)}\n'
                            f'Density-Based Calinski-Harabasz: {round(float(chi_dbcv), 4)}\n'
                            f'Density-Based CVDD: {round(float(cvdd_dbcv), 4)}\n'
                            f'Number of clusters: {len(set(labels))}')

            ax_right.text(0.1, 0.5, metrics_text, fontsize=22, verticalalignment='center',
              bbox=dict(facecolor='white', alpha=1, edgecolor='black', linewidth=2.5, pad=10))

        plt.tight_layout()
        pdf2.savefig(fig)
        plt.close(fig)

    return dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total

def generate_noisy_circles(n_datasets, n_samples, noise_increment, seed):
    datasets = []
    for i in range(n_datasets):
        noise_level = 0.05 + i * noise_increment
        noisy_circles = make_circles(n_samples=n_samples, factor=0.5, noise=noise_level, random_state=seed)
        datasets.append(noisy_circles[0])
    return datasets

def plot_datasets_with_clustering_noisy_circles(datasets):
    fig, axes = plt.subplots(5, 3, figsize=(30, 25))
    dbcv_total = []
    cvdd_dbcv_total = []
    chi_dbcv_total = []
    du_dbcv_total = []
    db_dbcv_total = []

    pdf_path1 = "clustering_plots_part1_noisy_circles.pdf"
    pdf_path2 = "clustering_plots_part2_noisy_circles.pdf"

    with PdfPages(pdf_path1) as pdf1, PdfPages(pdf_path2) as pdf2:
        for i, dataset in enumerate(datasets):
            if i == 5:
                plt.tight_layout()
                pdf1.savefig(fig)
                plt.close(fig)
                fig, axes = plt.subplots(5, 3, figsize=(30, 25))

            X = np.unique(dataset, axis=0)
            x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
            y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

            ax_left = axes[i % 5, 0]
            ax_left.scatter(X[:, 0], X[:, 1], c='black', s=10)
            ax_left.set_title(f'Dataset {i+1} - Original')
            ax_left.set_xlim(x_min, x_max)
            ax_left.set_ylim(y_min, y_max)
            ax_left.set_aspect('equal')
            ax_left.grid(True, color='lightgrey', linewidth=0.5)

            ax_middle = axes[i % 5, 1]
            clusterer = DBSCAN(eps=0.1, min_samples=10)
            labels = clusterer.fit_predict(X)

            scatter = ax_middle.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=5)
            ax_middle.set_title(f'Dataset {i+1} - DBSCAN')
            ax_middle.set_xlim(x_min, x_max)
            ax_middle.set_ylim(y_min, y_max)
            ax_middle.set_aspect('equal')
            ax_middle.grid(True, color='lightgrey', linewidth=0.5)

            dbcv_score = dbcvapi.s_dbcv(X, labels)
            dbcv_total.append(dbcv_score)

            db_dbcv = dbcvapi.db_dbcv(X, labels)
            db_dbcv_total.append(db_dbcv)

            du_dbcv = dbcvapi.di_dbcv(X, labels)
            du_dbcv_total.append(du_dbcv)

            chi_dbcv = dbcvapi.chi_dbcv(X, labels)
            chi_dbcv_total.append(chi_dbcv)

            cvdd_dbcv = dbcvapi.CVDDIndex(X, labels)
            cvdd_dbcv_total.append(cvdd_dbcv)

            num_clusters = len(set(labels)) - (1 if -1 in labels else 0)


            ax_right = axes[i % 5, 2]
            ax_right.axis('off')

            metrics_text = (f'DBCV: {round(float(dbcv_score), 4)}\n'
                            f'Density-Based Davies-Bouldin: {round(float(db_dbcv), 4)}\n'
                            f'Density-Based Dunn index: {round(float(du_dbcv), 4)}\n'
                            f'Density-Based Calinski-Harabasz: {round(float(chi_dbcv), 4)}\n'
                            f'Density-Based CVDD: {round(float(cvdd_dbcv), 4)}\n'
                            f'Number of clusters: {len(set(labels))}')

            ax_right.text(0.1, 0.5, metrics_text, fontsize=22, verticalalignment='center',
              bbox=dict(facecolor='white', alpha=1, edgecolor='black', linewidth=2.5, pad=10))


        plt.tight_layout()
        pdf2.savefig(fig)
        plt.close(fig)

    return dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total

def generate_augmented_datasets(seed):
    datasets = []
    noise_levels = [0.0, 0.056, 0.111, 0.167, 0.222, 0.278, 0.333, 0.389, 0.444, 0.5]

    for noise in noise_levels:
        np.random.seed(seed)
        num_new_points = 400
        new_points = normalized_points[np.random.choice(normalized_points.shape[0], num_new_points)]
        noisy_new_points = new_points + np.random.normal(0, noise, (num_new_points, 2))

        noisy_new_points = noisy_new_points % 1

        augmented_data = np.vstack((normalized_points, noisy_new_points))
        datasets.append(augmented_data)

    return datasets

def analyze_datasets_tulipan(datasets):
    results = []
    for X in datasets:
        dataset_results = []
        min_samples, epsilon = 35, 0.1
        clusterer = DBSCAN(min_samples=min_samples, eps=epsilon)
        labels = clusterer.fit_predict(X)

        dbcv_score = dbcvapi.s_dbcv(X, labels)
        db_dbcv = dbcvapi.db_dbcv(X, labels)
        du_dbcv = dbcvapi.di_dbcv(X, labels)
        chi_dbcv = dbcvapi.chi_dbcv(X, labels)
        cvdd_dbcv = dbcvapi.CVDDIndex(X, labels)


        dataset_results.append((dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv, labels))
        results.append(dataset_results)

    return results

def plot_datasets_with_clustering_tulipan(datasets, results):
    fig, axes = plt.subplots(5, 3, figsize=(18, 15))
    dbcv_total = []
    cvdd_dbcv_total = []
    chi_dbcv_total = []
    du_dbcv_total = []
    db_dbcv_total = []


    pdf_path1 = "clustering_plots_part1_tulipan.pdf"
    pdf_path2 = "clustering_plots_part2_tulipan.pdf"

    with PdfPages(pdf_path1) as pdf1, PdfPages(pdf_path2) as pdf2:
        for i, (dataset, metrics) in enumerate(zip(datasets, results)):
            if i == 5:
                plt.tight_layout()
                pdf1.savefig(fig)
                plt.close(fig)
                fig, axes = plt.subplots(5, 3, figsize=(18, 15))

            X = dataset
            dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv, labels = metrics[0]

            ax_left = axes[i % 5, 0]
            ax_left.scatter(X[:, 0], X[:, 1], c='black', s=10)
            ax_left.set_title(f'Dataset {i+1} - Original')
            ax_left.set_xlim(-0.1, 1.1)
            ax_left.set_ylim(-0.1, 1.1)
            ax_left.grid(True, color='lightgrey', linewidth=0.5)

            ax_middle = axes[i % 5, 1]
            ax_middle.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=5)
            ax_middle.set_title(f'Dataset {i+1} - DBSCAN')
            ax_middle.set_xlim(-0.1, 1.1)
            ax_middle.set_ylim(-0.1, 1.1)
            ax_middle.grid(True, color='lightgrey', linewidth=0.5)

            ax_right = axes[i % 5, 2]
            ax_right.axis('off')
            metrics_text = (f'DBCV: {round(float(dbcv_score), 4)}\n'
                            f'Density-Based Davies-Bouldin: {round(float(db_dbcv), 4)}\n'
                            f'Density-Based Dunn index: {round(float(du_dbcv), 4)}\n'
                            f'Density-Based Calinski-Harabasz: {round(float(chi_dbcv), 4)}\n'
                            f'Density-Based CVDD: {round(float(cvdd_dbcv), 4)}\n'
                            f'Number of clusters: {len(set(labels))}')

            ax_right.text(0.1, 0.5, metrics_text, fontsize=18, verticalalignment='center',
              bbox=dict(facecolor='white', alpha=1, edgecolor='black', linewidth=1.5, pad=5))

            dbcv_total.append(dbcv_score)
            db_dbcv_total.append(db_dbcv)
            du_dbcv_total.append(du_dbcv)
            chi_dbcv_total.append(chi_dbcv)
            cvdd_dbcv_total.append(cvdd_dbcv)

        plt.tight_layout()
        pdf2.savefig(fig)
        plt.close(fig)

    return dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total

def best_params_hdbscan(df):
  min_cluster_sizes = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,15]
  cluster_selection_epsilons = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.5]

  best_dbcv_score = -1
  best_params = None
  best_labels = None

  for min_cluster_size in min_cluster_sizes:
    for epsilon in cluster_selection_epsilons:

        hdbscan_val = HDBSCAN(min_cluster_size=min_cluster_size,
                             cluster_selection_epsilon=epsilon)

        labels = hdbscan_val.fit_predict(df)

        if len(set(labels)) > 0:
            dbcv_score = dbcv(df, labels)

            if dbcv_score > best_dbcv_score:
                best_dbcv_score = dbcv_score
                best_params = (min_cluster_size, epsilon)
                best_labels = labels


  print(f"min_cluster_size: {best_params[0]}, cluster_selection_epsilon: {best_params[1]}")

  return best_params[0],best_params[1]

def best_params_dbscan(df):
  min_cluster_sizes = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,15]
  cluster_selection_epsilons = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.5]

  best_dbcv_score = -1
  best_params = None
  best_labels = None

  for min_cluster_size in min_cluster_sizes:
    for epsilon in cluster_selection_epsilons:

        dbscan_pca = DBSCAN(min_samples=min_cluster_size,
                             eps=epsilon)

        labels = dbscan_pca.fit_predict(df)

        if len(set(labels)) > 0:
            dbcv_score = dbcv(df, labels)

            if dbcv_score > best_dbcv_score:
                best_dbcv_score = dbcv_score
                best_params = (min_cluster_size, epsilon)
                best_labels = labels

  print(f"min_cluster_size: {best_params[0]}, cluster_selection_epsilon: {best_params[1]}")
  return best_params[0], best_params[1]

def best_params_mean_shift(df):
  bandwidth_estimate = estimate_bandwidth(df, quantile=0.2, n_samples=500)
  bandwidth_range = np.linspace(bandwidth_estimate * 0.5, bandwidth_estimate * 1.5, 10)
  best_bandwidth = None
  best_score = -np.inf

  for bandwidth in bandwidth_range:
    ms = MeanShift(bandwidth=bandwidth)
    labels = ms.fit_predict(df)

    if len(set(labels)) > 1:
        try:

            dbcv_score = dbcv(df, labels)
            if dbcv_score > best_score:
                best_score = dbcv_score
                best_bandwidth = bandwidth
        except ValueError as e:
            print(f"Errore calcolando DBCV per banda {round(bandwidth,3)}: {e}")
    else:
        print(f"Banda {bandwidth} genera solo un cluster. Ignorato.")

    if best_bandwidth is not None:
        ms_final = MeanShift()
        labels_final = ms_final.fit_predict(df)
        final_score = dbcv(df, labels_final)

        if final_score > best_score:
            print(f"Best_score: {best_score}, Finale_score: {final_score}, best_bandwith: {best_bandwidth}")
            return None

    print(f"Best bandwidth: {round(best_bandwidth,3)}")
    return best_bandwidth

def similar_cluster(df):
  valori_unici_ms = df["Mean_shift_pred"].unique()
  valori_unici_db = df["DBSCAN_pred"].unique()
  valori_unici_hdb = df["HDBSCAN_pred"].unique()
  print(valori_unici_ms)
  print(valori_unici_db)
  print(valori_unici_hdb)

  mean_shift_labels = valori_unici_ms
  dbscan_labels = valori_unici_db
  hdbscan_labels = valori_unici_hdb
  total_rows = len(df)

  result = []

  for mean_label in mean_shift_labels:
    for dbscan_label in dbscan_labels:
        for hdbscan_label in hdbscan_labels:

            mask = (df['Mean_shift_pred'] == mean_label) & \
                   (df['DBSCAN_pred'] == dbscan_label) & \
                   (df['HDBSCAN_pred'] == hdbscan_label)
            count = mask.sum()
            percentage = (count / total_rows) * 100
            result.append({
                'Mean_shift': mean_label,
                'DBSCAN': dbscan_label,
                'HDBSCAN': hdbscan_label,
                'Percentuale': percentage
            })
  result_df = pd.DataFrame(result)
  result_df = result_df.sort_values(by='Percentuale', ascending=False)
  return result_df

def similar_cluster_default(df):
  valori_unici_ms = df["Mean_shift_default_pred"].unique()
  valori_unici_db = df["DBSCAN_default_pred"].unique()
  valori_unici_hdb = df["HDBSCAN_default_pred"].unique()
  print(valori_unici_ms)
  print(valori_unici_db)
  print(valori_unici_hdb)

  mean_shift_labels = valori_unici_ms
  dbscan_labels = valori_unici_db
  hdbscan_labels = valori_unici_hdb
  total_rows = len(df)

  result = []

  for mean_label in mean_shift_labels:
    for dbscan_label in dbscan_labels:
        for hdbscan_label in hdbscan_labels:

            mask = (df['Mean_shift_default_pred'] == mean_label) & \
                   (df['DBSCAN_default_pred'] == dbscan_label) & \
                   (df['HDBSCAN_default_pred'] == hdbscan_label)
            count = mask.sum()
            percentage = (count / total_rows) * 100
            result.append({
                'Mean_shift': mean_label,
                'DBSCAN': dbscan_label,
                'HDBSCAN': hdbscan_label,
                'Percentuale': percentage
            })
  result_df = pd.DataFrame(result)
  result_df = result_df.sort_values(by='Percentuale', ascending=False)
  return result_df

def compute_ari_default(df):
  ariAbs1 = abs(adjusted_rand_score(df["DBSCAN_default_pred"], df["HDBSCAN_default_pred"]))
  ariAbs2 = abs(adjusted_rand_score(df["HDBSCAN_default_pred"], df["Mean_shift_default_pred"]))
  ariAbs3 = abs(adjusted_rand_score(df["Mean_shift_default_pred"], df["DBSCAN_default_pred"]))

  ariAbsAverage = (ariAbs1 + ariAbs2 + ariAbs3) / 3

  print(f"ARI default: {round(ariAbsAverage,3)}, ariAbs1: {round(ariAbs1,3)}, ariAbs2: {round(ariAbs2,3)}, ariAbs3: {round(ariAbs3,3)}")
  return ariAbsAverage

def compute_ari(df):
  ariAbs1 = abs(adjusted_rand_score(df["DBSCAN_pred"], df["HDBSCAN_pred"]))
  ariAbs2 = abs(adjusted_rand_score(df["HDBSCAN_pred"], df["Mean_shift_pred"]))
  ariAbs3 = abs(adjusted_rand_score(df["Mean_shift_pred"], df["DBSCAN_pred"]))

  ariAbsAverage = (ariAbs1 + ariAbs2 + ariAbs3) / 3
  print(f"ARI default: {round(ariAbsAverage,3)}, ariAbs1: {round(ariAbs1,3)}, ariAbs2: {round(ariAbs2,3)}, ariAbs3: {round(ariAbs3,3)}")
  return ariAbsAverage

def compute_metrics(X, labels):
  dbcv_score = dbcvapi.s_dbcv(X, labels)
  db_dbcv = dbcvapi.db_dbcv(X, labels)
  du_dbcv = dbcvapi.di_dbcv(X, labels)
  chi_dbcv = dbcvapi.chi_dbcv(X, labels)
  cvdd_dbcv = dbcvapi.CVDDIndex(X, labels)

  return dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv

def calculate_trend_and_consistency(df,metric, method, default_method):
    best_value = df.loc[df["Method"] == method, metric].values[0]
    default_value = df.loc[df["Method"] == default_method, metric].values[0]

    if np.isnan(best_value) or np.isnan(default_value) or np.isinf(best_value) or np.isinf(default_value):
        return None, "Not Applicable"


    trend = "↑" if default_value > best_value else "↓"
    best_ari = df.loc[df["Method"] == method, "ARI"].values[0]
    default_ari = df.loc[df["Method"] == default_method, "ARI"].values[0]
    consistency = "Inconsistent" if (trend == "↑" and best_ari > default_ari) or (trend == "↓" and best_ari <= default_ari) else "Consistent"

    return trend, consistency

"""#**Find Best Params**"""

# start_time = time.time()
# datasets = generate_moon_datasets()
# results = analyze_datasets_best_params(datasets)
# best_params = find_best_params(results)
# plot_best_results(datasets, results, best_params)
# end_time = time.time()

# print(f"Tempo totale di esecuzione: {end_time - start_time:.2f} secondi")

"""#**Artificial Datasets**

**Half Moons**
"""

datasets = generate_moon_datasets(30)

dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total = plot_datasets_with_clustering(datasets)
noise = [0.0, 0.056, 0.111, 0.167, 0.222, 0.278, 0.333, 0.389, 0.444, 0.5]

dbcv_total = list(zip(dbcv_total, noise))
db_dbcv_total = list(zip(db_dbcv_total, noise))
du_dbcv_total = list(zip(du_dbcv_total, noise))
chi_dbcv_total = list(zip(chi_dbcv_total, noise))
cvdd_dbcv_total = list(zip(cvdd_dbcv_total, noise))


data_list = [
    ("DBCV", dbcv_total),
    ("DBDBI", db_dbcv_total),
    ("DBDI", du_dbcv_total),
    ("DBCHI", chi_dbcv_total),
    ("CVDD", cvdd_dbcv_total)

]
plt.figure(figsize=(15, 10))

for i, (name, data) in enumerate(data_list):
    plt.subplot(4, 2, i + 1)
    y_values = [val[0] for val in data]
    x_values = [val[1] for val in data]

    plt.plot(x_values, y_values, marker='o', linestyle='-', color='b')
    plt.title(name)

    plt.xlabel('Noise')
    plt.ylabel('Value')
    plt.xticks(x_values)

    plt.grid(True)
    plt.gca().set_facecolor('#f0f0f0')

    if name in ["DBCV", "Silhouette"]:
        plt.ylim(-1, 1)

#plt.suptitle("Performance Metrics vs Noise in Half Moons Datasets", fontsize=16, fontweight='bold')

plt.tight_layout()

pdf_path = "metrics_vs_noise.pdf"
with PdfPages(pdf_path) as pdf:
    pdf.savefig(plt.gcf())
plt.close()

print(f"PDF saved at: {pdf_path}")

tolerance_times = 3
val_tolerence = 0.05
dict_decrease = {}
dict_moon = {}
dict_decreasing_total={}

for name, data in data_list:
  decreasing_count = 0
  equal_count = 0
  x_values = [val[0] for val in data]
  for i in range(len(x_values) - 1):
    if x_values[i] >= x_values[i+1]:
      decreasing_count += 1
    if x_values[i] == x_values[i+1]:
      equal_count += 1
  if (decreasing_count >= len(x_values) - tolerance_times) and equal_count < (len(x_values)-1):
    dict_moon[name] = 1
  else:
    dict_moon[name] = 0
  print(f"for {name}, Decreasing count: {decreasing_count}, Equal count: {equal_count}")
  dict_decreasing_total[name] = (decreasing_count - equal_count)



dict_decrease["Half Moons"] = dict_moon
dict_decrease

"""**Shifting Circles**"""

n_datasets = 10
n_points_per_circle = 500
n_points_per_circle_inner = 500
inner_radius = 0.5
outer_radius = 1.5
initial_shift = (0, 0)
shift_increment = (0.2, 0)
datasets = generate_datasets_with_two_circles(n_datasets, n_points_per_circle, n_points_per_circle_inner, inner_radius, outer_radius, initial_shift, shift_increment)

dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total = plot_datasets_with_clustering_circle(datasets)

noise = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8]

dbcv_total = list(zip(dbcv_total, noise))
db_dbcv_total = list(zip(db_dbcv_total, noise))
du_dbcv_total = list(zip(du_dbcv_total, noise))
chi_dbcv_total = list(zip(chi_dbcv_total, noise))
cvdd_dbcv_total = list(zip(cvdd_dbcv_total, noise))

data_list = [
    ("DBCV", dbcv_total),
    ("DBDBI", db_dbcv_total),
    ("DBDI", du_dbcv_total),
    ("DBCHI", chi_dbcv_total),
    ("CVDD", cvdd_dbcv_total)

]

plt.figure(figsize=(15, 10))

for i, (name, data) in enumerate(data_list):
    plt.subplot(4, 2, i + 1)
    y_values = [val[0] for val in data]
    x_values = [val[1] for val in data]

    plt.plot(x_values, y_values, marker='o', linestyle='-', color='b')
    plt.title(name)

    plt.xlabel('Noise')
    plt.ylabel('Value')
    plt.xticks(x_values)

    plt.grid(True)
    plt.gca().set_facecolor('#f0f0f0')

    if name in ["DBCV", "Silhouette"]:
        plt.ylim(-1, 1)

#plt.suptitle("Performance Metrics vs Noise in Shifting Cirlces Datasets", fontsize=16, fontweight='bold')

plt.tight_layout()

pdf_path = "metrics_vs_noise_shifting_circle.pdf"
with PdfPages(pdf_path) as pdf:
    pdf.savefig(plt.gcf())
plt.close()

print(f"PDF saved at: {pdf_path}")

dict_shifting_circle = {}

for name, data in data_list:
  decreasing_count = 0
  equal_count = 0
  x_values = [val[0] for val in data]
  for i in range(len(x_values) - 1):
    if x_values[i] >= x_values[i+1]:
      decreasing_count += 1
    if x_values[i] == x_values[i+1]:
      equal_count += 1
  if (decreasing_count >= len(x_values) - tolerance_times) and equal_count < (len(x_values)-1):
    dict_shifting_circle[name] = 1
  else:
    dict_shifting_circle[name] = 0
  print(f"for {name}, Decreasing count: {decreasing_count}, Equal count: {equal_count}")
  dict_decreasing_total[name] += (decreasing_count - equal_count)

dict_decrease["Shifting Circles"] = dict_shifting_circle
dict_decrease

"""**Sparse Circles**"""

n_datasets = 10
n_samples = 1000
noise_increment = 0.0555
seed = 4

#noise_levels = [0.0, 0.056, 0.111, 0.167, 0.222, 0.278, 0.333, 0.389, 0.444, 0.5]


datasets = generate_noisy_circles(n_datasets, n_samples, noise_increment, seed)


dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total  = plot_datasets_with_clustering_noisy_circles(datasets)
noise_level = [i * noise_increment for i in range(n_datasets)]

dbcv_total = list(zip(dbcv_total, noise))
db_dbcv_total = list(zip(db_dbcv_total, noise))
du_dbcv_total = list(zip(du_dbcv_total, noise))
chi_dbcv_total = list(zip(chi_dbcv_total, noise))
cvdd_dbcv_total = list(zip(cvdd_dbcv_total, noise))


data_list = [
    ("DBCV", dbcv_total),
    ("DBDBI", db_dbcv_total),
    ("DBDI", du_dbcv_total),
    ("DBCHI", chi_dbcv_total),
    ("CVDD", cvdd_dbcv_total)

]

plt.figure(figsize=(15, 10))

for i, (name, data) in enumerate(data_list):
    plt.subplot(4, 2, i + 1)
    y_values = [val[0] for val in data]
    x_values = [val[1] for val in data]

    plt.plot(x_values, y_values, marker='o', linestyle='-', color='b')
    plt.title(name)

    plt.xlabel('Noise')
    plt.ylabel('Value')
    plt.xticks(x_values)

    plt.grid(True)
    plt.gca().set_facecolor('#f0f0f0')

    if name in ["DBCV", "Silhouette"]:
        plt.ylim(-1, 1)

#plt.suptitle("Performance Metrics vs Noise in Sparse Cirlces Datasets", fontsize=16, fontweight='bold')

plt.tight_layout()

pdf_path = "metrics_vs_noise_sparse_circle.pdf"
with PdfPages(pdf_path) as pdf:
    pdf.savefig(plt.gcf())
plt.close()

print(f"PDF saved at: {pdf_path}")

dict_sparse_circle = {}

for name, data in data_list:
  decreasing_count = 0
  equal_count = 0
  x_values = [val[0] for val in data]
  for i in range(len(x_values) - 1):
    if x_values[i] >= x_values[i+1]:
      decreasing_count += 1
    if x_values[i] == x_values[i+1]:
      equal_count += 1
  if (decreasing_count >= len(x_values) - tolerance_times) and equal_count < (len(x_values)-1):
    dict_sparse_circle[name] = 1
  else:
    dict_sparse_circle[name] = 0
  print(f"for {name}, Decreasing count: {decreasing_count}, Equal count: {equal_count}")
  dict_decreasing_total[name] += (decreasing_count - equal_count)

dict_decrease["Sparse Circles"] = dict_sparse_circle
dict_decrease

"""**Tulipan**"""

data = scipy.io.loadmat('/content/Moon.mat')
X = data['data'][:, 0]
Y = data['data'][:, 1]
original_points = np.column_stack((X, Y))

min_vals = original_points.min(axis=0)
max_vals = original_points.max(axis=0)
normalized_points = (original_points - min_vals) / (max_vals - min_vals)

datasets = generate_augmented_datasets(30)
results = analyze_datasets_tulipan(datasets)
dbcv_total, db_dbcv_total, du_dbcv_total, chi_dbcv_total, cvdd_dbcv_total  = plot_datasets_with_clustering_tulipan(datasets, results)

noise_levels = [0.0, 0.056, 0.111, 0.167, 0.222, 0.278, 0.333, 0.389, 0.444, 0.5]

dbcv_total = list(zip(dbcv_total, noise))
db_dbcv_total = list(zip(db_dbcv_total, noise))
du_dbcv_total = list(zip(du_dbcv_total, noise))
chi_dbcv_total = list(zip(chi_dbcv_total, noise))
cvdd_dbcv_total = list(zip(cvdd_dbcv_total, noise))


data_list = [
    ("DBCV", dbcv_total),
    ("DBDBI", db_dbcv_total),
    ("DBDI", du_dbcv_total),
    ("DBCHI", chi_dbcv_total),
    ("CVDD", cvdd_dbcv_total)

]

plt.figure(figsize=(15, 10))

for i, (name, data) in enumerate(data_list):
    plt.subplot(4, 2, i + 1)
    y_values = [val[0] for val in data]
    x_values = [val[1] for val in data]

    plt.plot(x_values, y_values, marker='o', linestyle='-', color='b')
    plt.title(name)

    plt.xlabel('Noise')
    plt.ylabel('Value')
    plt.xticks(x_values)

    plt.grid(True)
    plt.gca().set_facecolor('#f0f0f0')

    if name in ["DBCV", "Silhouette"]:
        plt.ylim(-1, 1)

#plt.suptitle("Performance Metrics vs Noise in Tulipan Datasets", fontsize=16, fontweight='bold')

plt.tight_layout()

pdf_path = "metrics_vs_noise_tulipan.pdf"
with PdfPages(pdf_path) as pdf:
    pdf.savefig(plt.gcf())
plt.close()

print(f"PDF saved at: {pdf_path}")

dict_tulipan = {}

for name, data in data_list:
  decreasing_count = 0
  equal_count = 0
  x_values = [val[0] for val in data]
  for i in range(len(x_values) - 1):
    if x_values[i] >= x_values[i+1]:
      decreasing_count += 1
    if x_values[i] == x_values[i+1]:
      equal_count += 1
  if (decreasing_count >= len(x_values) - tolerance_times) and equal_count < (len(x_values)-1):
    dict_tulipan[name] = 1
  else:
    dict_tulipan[name] = 0
  print(f"for {name}, Decreasing count: {decreasing_count}, Equal count: {equal_count}")
  dict_decreasing_total[name] += (decreasing_count - equal_count)

dict_decrease["Tulipan"] = dict_tulipan
dict_decrease

df_decrease = pd.DataFrame(dict_decrease)
df_decrease['Total'] = df_decrease.sum(axis=1)
df_decrease = df_decrease.sort_values(by='Total', ascending=False)

print("Tab.1 Improving table:\n")
df_decrease

df_decreasing_total = pd.DataFrame.from_dict(dict_decreasing_total, orient='index', columns=['Decreasing Trend Count'])
df_decreasing_total
#Ordinamento decrescente

"""#**EHRs**"""

neuroblastoma=pd.read_csv("/content/10_7717_peerj_5665_dataYM2018_neuroblastoma.csv")
diabetes=pd.read_csv("/content/journal.pone.0216416_Takashi2019_diabetes_type1_dataset_preprocessed.csv")
sepsis=pd.read_csv("/content/journal.pone.0148699_S1_Text_Sepsis_SIRS_EDITED.csv")
heart_failure=pd.read_csv("/content/journal.pone.0158570_S2File_depression_heart_failure_v2.csv")
cardiac_arrest=pd.read_csv("/content/journal.pone.0175818_S1Dataset_Spain_cardiac_arrest_EDITED..csv")
df_metrics=pd.DataFrame()


clustering_methods = ["HDBSCAN", "DBSCAN", "Meanshift", "HDBSCAN_default", "DBSCAN_default", "Meanshift_default"]

pairs = [
    ("HDBSCAN", "HDBSCAN_default"),
    ("DBSCAN", "DBSCAN_default"),
    ("Meanshift", "Meanshift_default")
]

"""#**Neuroblastoma**"""

neuroblastoma = neuroblastoma.dropna(thresh=neuroblastoma.shape[1] - 2)
numerical_features = neuroblastoma.columns
scaler_minmax = MinMaxScaler()
neuroblastoma_scaled = neuroblastoma.copy()
neuroblastoma_scaled[numerical_features] = scaler_minmax.fit_transform(neuroblastoma[numerical_features])
neuroblastoma_scaled.reset_index(drop=True, inplace=True)
neuroblastoma_scaled.drop_duplicates(inplace=True)
df_metrics_neuroblastoma = pd.DataFrame()
neuroblastoma_scaled

"""**HDBSCAN**"""

if isinstance(neuroblastoma_scaled, pd.DataFrame):
    X_array = neuroblastoma_scaled.values
else:
    X_array = neuroblastoma_scaled


random.seed(51)
min_size = random.randint(5, 20)
epsilon = random.uniform(0.1, 1.0)
print(f"min_size: {min_size}, epsilon: {epsilon}")

hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=min_size, cluster_selection_epsilon=epsilon)
hdbscan_cluster_default = hdbscan.HDBSCAN()

labels = hdbscan_cluster.fit_predict(neuroblastoma_scaled)
labels_default = hdbscan_cluster_default.fit_predict(neuroblastoma_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(X_array, labels)
neuroblastoma_scaled['HDBSCAN_pred'] = labels

metrics_dict = {
    'Method': 'HDBSCAN',
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}


dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default = compute_metrics(X_array, labels_default)
neuroblastoma_scaled['HDBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': 'HDBSCAN_default',
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_neuroblastoma = pd.concat([df_metrics_neuroblastoma, metrics_df], ignore_index=True)
df_metrics_neuroblastoma = pd.concat([df_metrics_neuroblastoma, metrics_df_default], ignore_index=True)

df_metrics_neuroblastoma

"""**DBSCAN**"""

#min_sample, eps = best_params_dbscan(neuroblastoma_scaled)


random.seed(2)
min_sample = random.randint(2, 20)
eps = random.uniform(0.1, 2.0)
print(f"min_sample: {min_sample}, eps: {eps}" )

dbscan = DBSCAN(min_samples=min_sample, eps=eps)
dbscan_default = DBSCAN()

labels = dbscan.fit_predict(neuroblastoma_scaled)
labels_default = dbscan_default.fit_predict(neuroblastoma_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv  = compute_metrics(X_array, labels)
neuroblastoma_scaled['DBSCAN_pred'] = labels

methods = 'DBSCAN'
methods_default = 'DBSCAN_default'
metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default = compute_metrics(X_array, labels_default)
neuroblastoma_scaled['DBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_neuroblastoma = pd.concat([df_metrics_neuroblastoma, metrics_df], ignore_index=True)
df_metrics_neuroblastoma = pd.concat([df_metrics_neuroblastoma, metrics_df_default], ignore_index=True)

"""**Meanshift**"""

random.seed(9)
band = random.uniform(0.1, 5.0)
print(f"band: {band}" )

mean_shift = MeanShift(bandwidth=band)
mean_shift_default = MeanShift()

labels = mean_shift.fit_predict(neuroblastoma_scaled)
labels_default = mean_shift_default.fit_predict(neuroblastoma_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(X_array, labels)
neuroblastoma_scaled['Mean_shift_pred'] = labels

methods = 'Meanshift'
methods_default = 'Meanshift_default'
metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default = compute_metrics(X_array, labels_default)
neuroblastoma_scaled['Mean_shift_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_neuroblastoma = pd.concat([df_metrics_neuroblastoma, metrics_df], ignore_index=True)
df_metrics_neuroblastoma = pd.concat([df_metrics_neuroblastoma, metrics_df_default], ignore_index=True)
print("Tab.2 Metrics for neuroblastoma:\n")
df_metrics_neuroblastoma

"""**Cluster Analysis**"""

cluster_similarity = similar_cluster(neuroblastoma_scaled)
cluster_similarity

cluster_similarity_default = similar_cluster_default(neuroblastoma_scaled)
cluster_similarity_default

neuroblastoma_scaled["DBSCAN_pred"] = neuroblastoma_scaled["DBSCAN_pred"].astype(object)
neuroblastoma_scaled["HDBSCAN_pred"] = neuroblastoma_scaled["HDBSCAN_pred"].astype(object)
neuroblastoma_scaled["Mean_shift_pred"] = neuroblastoma_scaled["Mean_shift_pred"].astype(object)

neuroblastoma_scaled.loc[neuroblastoma_scaled["DBSCAN_pred"] == -1, "DBSCAN_pred"] = 'A'
neuroblastoma_scaled.loc[neuroblastoma_scaled["HDBSCAN_pred"] == 1, "HDBSCAN_pred"] = 'A'
neuroblastoma_scaled.loc[neuroblastoma_scaled["Mean_shift_pred"] == 0, "Mean_shift_pred"] = 'A'

neuroblastoma_scaled.loc[neuroblastoma_scaled["DBSCAN_pred"] == 8, "DBSCAN_pred"] = 'B'
neuroblastoma_scaled.loc[neuroblastoma_scaled["HDBSCAN_pred"] == 0, "HDBSCAN_pred"] = 'B'
neuroblastoma_scaled.loc[neuroblastoma_scaled["Mean_shift_pred"] == 4, "Mean_shift_pred"] = 'B'


neuroblastoma_scaled.loc[neuroblastoma_scaled["DBSCAN_default_pred"] == -1, "DBSCAN_default_pred"] = 'A'
neuroblastoma_scaled.loc[neuroblastoma_scaled["HDBSCAN_default_pred"] == 1, "HDBSCAN_default_pred"] = 'A'
neuroblastoma_scaled.loc[neuroblastoma_scaled["Mean_shift_default_pred"] == 0, "Mean_shift_default_pred"] = 'A'

neuroblastoma_scaled.loc[neuroblastoma_scaled["DBSCAN_default_pred"] == 1, "DBSCAN_default_pred"] = 'B'
neuroblastoma_scaled.loc[neuroblastoma_scaled["HDBSCAN_default_pred"] == 0, "HDBSCAN_default_pred"] = 'B'
neuroblastoma_scaled.loc[neuroblastoma_scaled["Mean_shift_default_pred"] == 3, "Mean_shift_default_pred"] = 'B'

neuroblastoma_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]] = (
    neuroblastoma_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]]
    .replace({'A': 0,'B':1})
    .astype(int)
)

neuroblastoma_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]] = (
    neuroblastoma_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]]
    .replace({'A': 0, 'B':1})
    .astype(int)
)

same_value_count = (neuroblastoma_scaled["HDBSCAN_pred"] == neuroblastoma_scaled["DBSCAN_pred"]) & \
                   (neuroblastoma_scaled["DBSCAN_pred"] == neuroblastoma_scaled["Mean_shift_pred"])

same_value_count_default = (neuroblastoma_scaled["HDBSCAN_default_pred"] == neuroblastoma_scaled["DBSCAN_default_pred"]) & \
                   (neuroblastoma_scaled["DBSCAN_default_pred"] == neuroblastoma_scaled["Mean_shift_default_pred"])

total_same_value = same_value_count.sum()
total_same_value_default = same_value_count_default.sum()
total_rows = len(neuroblastoma_scaled)

percentage_same_value = (total_same_value / total_rows) * 100
print("Totale delle righe con lo stesso valore:", total_same_value)
print("Percentuale rispetto al totale:", round(percentage_same_value,2), "%")
percentage_same_value = round((percentage_same_value / 100),3)
ari = compute_ari(neuroblastoma_scaled)


percentage_same_value_default = (total_same_value_default / total_rows) * 100
print("Totale delle righe con lo stesso valore con parametri default:", total_same_value_default)
print("Percentuale rispetto al totale:", round(percentage_same_value_default,2), "%")
percentage_same_value_default = round((percentage_same_value_default / 100),3)
ari_default = compute_ari_default(neuroblastoma_scaled)

df_metrics_neuroblastoma['Clustering Truth'] = df_metrics_neuroblastoma['Method'].apply(
    lambda x: percentage_same_value_default if 'default' in x else percentage_same_value)

df_metrics_neuroblastoma['ARI'] = df_metrics_neuroblastoma['Method'].apply(
    lambda x: ari_default if 'default' in x else ari)

df_metrics_neuroblastoma = df_metrics_neuroblastoma.sort_values('DBCV', ascending=False)

df_metrics_neuroblastoma

results = []
methods = ["DBSCAN", "HDBSCAN", "Meanshift"]
default_methods = {
    "DBSCAN": "DBSCAN_default",
    "HDBSCAN": "HDBSCAN_default",
    "Meanshift": "Meanshift_default"
}
metrics_distance = [
    "DBCV", "DBDBI",	"DBDI",	"DBCHI",	"CVDD"
]



for method in methods:
    for metric_value in metrics_distance:
        if metric_value in df_metrics_neuroblastoma.columns:
            best_value = df_metrics_neuroblastoma.loc[df_metrics_neuroblastoma["Method"] == method, metric_value].values[0]
            default_value = df_metrics_neuroblastoma.loc[df_metrics_neuroblastoma["Method"] == default_methods[method], metric_value].values[0]
            trend, consistency = calculate_trend_and_consistency(df_metrics_neuroblastoma,metric_value, method, default_methods[method])
            results.append({
                "Metric": metric_value.split(" ")[0],
                "Random_parameters": round(best_value,3),
                "Default": round(default_value,3),
                "Method": method,
                "Random_ARI": round(ari,3),
                "Default_ARI": round(ari_default,3),
                "trend": trend,
                "Trend consistency with ARI": consistency
            })

final_table = pd.DataFrame(results)
print("Tab.3 Trend Conisentecy With ARI :\n")
final_table

consistency_table = final_table.groupby("Metric")["Trend consistency with ARI"].value_counts().unstack(fill_value=0)
consistency_table = consistency_table.rename(columns={"Consistent": "N°Consistent", "Inconsistent": "N°Inconsistent"}).reset_index()
consistency_table = consistency_table.sort_values(by="N°Consistent", ascending=False).reset_index(drop=True)
final_summary_table = consistency_table
consistency_table

"""#**Diabetes**"""

diabetes = diabetes.dropna(thresh=diabetes.shape[1] - 2)
numerical_features = diabetes.columns
scaler_minmax = MinMaxScaler()
diabetes_scaled = diabetes.copy()
diabetes_scaled[numerical_features] = scaler_minmax.fit_transform(diabetes[numerical_features])
diabetes_scaled.reset_index(drop=True, inplace=True)
diabetes_scaled.drop_duplicates(inplace=True)
df_metrics_diabetes = pd.DataFrame()
diabetes_scaled

"""**HDBSCAN**"""

#min_size, epsilon= best_params_hdbscan(diabetes_scaled)

if isinstance(diabetes_scaled, pd.DataFrame):
    X_array = diabetes_scaled.values
else:
    X_array = diabetes_scaled

random.seed(5)
min_size = random.randint(5, 20)
epsilon = random.uniform(0.01, 1.0)
print(f"min_size: {min_size}, epsilon: {epsilon}" )

hdbscan_cluster= hdbscan.HDBSCAN(min_cluster_size=min_size, cluster_selection_epsilon=epsilon)
hdbscan_cluster_default= hdbscan.HDBSCAN()

labels = hdbscan_cluster.fit_predict(diabetes_scaled)
labels_default = hdbscan_cluster_default.fit_predict(diabetes_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv  = compute_metrics(X_array, labels)
diabetes_scaled['HDBSCAN_pred'] = labels

methods = 'HDBSCAN'
methods_default = 'HDBSCAN_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default  = compute_metrics(X_array, labels_default)
diabetes_scaled['HDBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_diabetes = pd.concat([df_metrics_diabetes, metrics_df], ignore_index=True)
df_metrics_diabetes = pd.concat([df_metrics_diabetes, metrics_df_default], ignore_index=True)
df_metrics_diabetes

"""**DBSCAN**"""

random.seed(1)
min_sample = random.randint(2, 20)
eps = random.uniform(0.1, 2.0)
print(f"min_sample: {min_sample}, eps: {eps}" )

dbscan = DBSCAN(min_samples=min_sample, eps=eps)
dbscan_default = DBSCAN()

labels = dbscan.fit_predict(diabetes_scaled)
labels_default = dbscan_default.fit_predict(diabetes_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(X_array, labels)
diabetes_scaled['DBSCAN_pred'] = labels

methods = 'DBSCAN'
methods_default = 'DBSCAN_default'
metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default= compute_metrics(X_array, labels_default)
diabetes_scaled['DBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_diabetes = pd.concat([df_metrics_diabetes, metrics_df], ignore_index=True)
df_metrics_diabetes = pd.concat([df_metrics_diabetes, metrics_df_default], ignore_index=True)
df_metrics_diabetes

"""**Mean_Shift**"""

random.seed(9)
band = random.uniform(0.1, 5.0)
print(f"band: {band}" )

mean_shift = MeanShift(bandwidth=band)
mean_shift_default = MeanShift()

labels = mean_shift.fit_predict(diabetes_scaled)
labels_default = mean_shift_default.fit_predict(diabetes_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv  = compute_metrics(X_array, labels)
diabetes_scaled['Mean_shift_pred'] = labels

methods = 'Meanshift'
methods_default = 'Meanshift_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default  = compute_metrics(X_array, labels_default)
diabetes_scaled['Mean_shift_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_diabetes = pd.concat([df_metrics_diabetes, metrics_df], ignore_index=True)
df_metrics_diabetes = pd.concat([df_metrics_diabetes, metrics_df_default], ignore_index=True)
df_metrics_diabetes

"""**Cluster Analysis**"""

cluster_similarity = similar_cluster(diabetes_scaled)
cluster_similarity

cluster_similarity_default = similar_cluster_default(diabetes_scaled)
cluster_similarity_default

diabetes_scaled["DBSCAN_pred"] = diabetes_scaled["DBSCAN_pred"].astype(object)
diabetes_scaled["HDBSCAN_pred"] = diabetes_scaled["HDBSCAN_pred"].astype(object)
diabetes_scaled["Mean_shift_pred"] = diabetes_scaled["Mean_shift_pred"].astype(object)

diabetes_scaled.loc[diabetes_scaled["DBSCAN_pred"] == 1, "DBSCAN_pred"] = 'A'
diabetes_scaled.loc[diabetes_scaled["HDBSCAN_pred"] == 0, "HDBSCAN_pred"] = 'A'
diabetes_scaled.loc[diabetes_scaled["Mean_shift_pred"] == 0, "Mean_shift_pred"] = 'A'

diabetes_scaled.loc[diabetes_scaled["DBSCAN_pred"] == -1, "DBSCAN_pred"] = 'B'
diabetes_scaled.loc[diabetes_scaled["HDBSCAN_pred"] == -1, "HDBSCAN_pred"] = 'B'
diabetes_scaled.loc[diabetes_scaled["Mean_shift_pred"] == 1, "Mean_shift_pred"] = 'B'


diabetes_scaled.loc[diabetes_scaled["DBSCAN_default_pred"] == -1, "DBSCAN_default_pred"] = 'A'
diabetes_scaled.loc[diabetes_scaled["HDBSCAN_default_pred"] == 2, "HDBSCAN_default_pred"] = 'A'
diabetes_scaled.loc[diabetes_scaled["Mean_shift_default_pred"] == 0, "Mean_shift_default_pred"] = 'A'

diabetes_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]] = (
    diabetes_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]]
    .replace({'A': 0, 'B': 1})
    .astype(int)
)

diabetes_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]] = (
    diabetes_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]]
    .replace({'A': 0})
    .astype(int)
)

same_value_count = (diabetes_scaled["HDBSCAN_pred"] == diabetes_scaled["DBSCAN_pred"]) & \
                   (diabetes_scaled["DBSCAN_pred"] == diabetes_scaled["Mean_shift_pred"])

same_value_count_default = (diabetes_scaled["HDBSCAN_default_pred"] == diabetes_scaled["DBSCAN_default_pred"]) & \
                   (diabetes_scaled["DBSCAN_default_pred"] == diabetes_scaled["Mean_shift_default_pred"])

total_same_value = same_value_count.sum()
total_same_value_default = same_value_count_default.sum()
total_rows = len(diabetes_scaled)

percentage_same_value = (total_same_value / total_rows) * 100
print("Totale delle righe con lo stesso valore:", total_same_value)
print("Percentuale rispetto al totale:", round(percentage_same_value,2), "%")
percentage_same_value = round((percentage_same_value / 100),3)
ari = compute_ari(diabetes_scaled)

percentage_same_value_default = (total_same_value_default / total_rows) * 100
print("Totale delle righe con lo stesso valore con parametri default:", total_same_value_default)
print("Percentuale rispetto al totale:", round(percentage_same_value_default,2), "%")
percentage_same_value_default = round((percentage_same_value_default / 100),3)
ari_default = compute_ari_default(diabetes_scaled)

df_metrics_diabetes['Clustering Truth'] = df_metrics_diabetes['Method'].apply(
    lambda x: percentage_same_value_default if 'default' in x else percentage_same_value)

df_metrics_diabetes['ARI'] = df_metrics_diabetes['Method'].apply(
    lambda x: ari_default if 'default' in x else ari)

df_metrics_diabetes = df_metrics_diabetes.sort_values('DBCV', ascending=False)

df_metrics_diabetes

results = []
methods = ["DBSCAN", "HDBSCAN", "Meanshift"]
default_methods = {
    "DBSCAN": "DBSCAN_default",
    "HDBSCAN": "HDBSCAN_default",
    "Meanshift": "Meanshift_default"
}
metrics_distance = [
    "DBCV", "DBDBI",	"DBDI",	"DBCHI",	"CVDD"
]



for method in methods:
    for metric_value in metrics_distance:
        if metric_value in df_metrics_diabetes.columns:
            best_value = df_metrics_diabetes.loc[df_metrics_diabetes["Method"] == method, metric_value].values[0]
            default_value = df_metrics_diabetes.loc[df_metrics_diabetes["Method"] == default_methods[method], metric_value].values[0]
            trend, consistency = calculate_trend_and_consistency(df_metrics_diabetes,metric_value, method, default_methods[method])
            results.append({
                "Metric": metric_value.split(" ")[0],
                "Random_parameters": round(best_value,3),
                "Default": round(default_value,3),
                "Method": method,
                "Random_ARI": round(ari,3),
                "Default_ARI": round(ari_default,3),
                "trend": trend,
                "Trend consistency with ARI": consistency
            })

final_table = pd.DataFrame(results)
print("Tab.5 Trend Conisentecy With ARI :\n")
final_table

consistency_table = final_table.groupby("Metric")["Trend consistency with ARI"].value_counts().unstack(fill_value=0)
consistency_table = consistency_table.rename(columns={"Consistent": "N°Consistent", "Inconsistent": "N°Inconsistent"}).reset_index()
consistency_table = consistency_table.sort_values(by="N°Consistent", ascending=False).reset_index(drop=True)
#consistency_table = consistency_table.to_string(index=False)
final_summary_table = final_summary_table.merge(consistency_table, on="Metric", how="outer", suffixes=('', '_new')).fillna(0)
final_summary_table["N°Consistent"] += final_summary_table.pop("N°Consistent_new")
final_summary_table["N°Inconsistent"] += final_summary_table.pop("N°Inconsistent_new")
final_summary_table["Not Applicable"] += final_summary_table.pop("Not Applicable_new")
consistency_table

"""#**Sepsis**"""

sepsis = sepsis.dropna(thresh=sepsis.shape[1] - 2)
numerical_features = sepsis.columns
scaler_minmax = MinMaxScaler()
sepsis_scaled = sepsis.copy()
sepsis_scaled[numerical_features] = scaler_minmax.fit_transform(sepsis_scaled[numerical_features])
sepsis_scaled.reset_index(drop=True, inplace=True)
sepsis_scaled.drop_duplicates(inplace=True)
df_metrics_sepsis = pd.DataFrame()
sepsis_scaled

"""**HDBSCAN**"""

#min_size, epsilon= best_params_hdbscan(sepsis_scaled)
if isinstance(sepsis_scaled, pd.DataFrame):
    X_array = sepsis_scaled.values
else:
    X_array = sepsis_scaled

random.seed(45)
min_size = random.randint(5, 20)
epsilon = random.uniform(0.01, 1.0)
print(f"min_size: {min_size}, epsilon: {epsilon}" )

hdbscan_cluster= hdbscan.HDBSCAN(min_cluster_size=min_size, cluster_selection_epsilon=epsilon)
hdbscan_cluster_default= hdbscan.HDBSCAN()

labels = hdbscan_cluster.fit_predict(sepsis_scaled)
labels_default = hdbscan_cluster_default.fit_predict(sepsis_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(X_array, labels)
sepsis_scaled['HDBSCAN_pred'] = labels

methods = 'HDBSCAN'
methods_default = 'HDBSCAN_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default = compute_metrics(X_array, labels_default)
sepsis_scaled['HDBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_sepsis = pd.concat([df_metrics_sepsis, metrics_df], ignore_index=True)
df_metrics_sepsis = pd.concat([df_metrics_sepsis, metrics_df_default], ignore_index=True)
df_metrics_sepsis

"""**DBSCAN**"""

random.seed(46)
min_sample = random.randint(2, 20)
eps = random.uniform(0.1, 2.0)
print(f"min_sample: {min_sample}, eps: {eps}" )

dbscan = DBSCAN(min_samples=min_sample, eps=eps)
dbscan_default = DBSCAN()

labels = dbscan.fit_predict(sepsis_scaled)
labels_default = dbscan_default.fit_predict(sepsis_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(sepsis_scaled.values, labels)

sepsis_scaled['DBSCAN_pred'] = labels

methods = 'DBSCAN'
methods_default = 'DBSCAN_default'

metrics_dict = {
    'Method':methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}


dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default = compute_metrics(X_array, labels_default)
sepsis_scaled['DBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_sepsis = pd.concat([df_metrics_sepsis, metrics_df], ignore_index=True)
df_metrics_sepsis = pd.concat([df_metrics_sepsis, metrics_df_default], ignore_index=True)
df_metrics_sepsis

"""**Mean_Shift**"""

random.seed(45)
band = random.uniform(0.1, 5.0)
print(f"band: {band}" )

mean_shift = MeanShift(bandwidth=band)
mean_shift_default = MeanShift()

labels = mean_shift.fit_predict(sepsis_scaled)
labels_default = mean_shift_default.fit_predict(sepsis_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(X_array, labels)
sepsis_scaled['Mean_shift_pred'] = labels


methods = 'Meanshift'
methods_default = 'Meanshift_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}


dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default  = compute_metrics(X_array, labels_default)
sepsis_scaled['Mean_shift_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_sepsis = pd.concat([df_metrics_sepsis, metrics_df], ignore_index=True)
df_metrics_sepsis = pd.concat([df_metrics_sepsis, metrics_df_default], ignore_index=True)
df_metrics_sepsis

"""**Cluster Analysis**"""

cluster_similarity = similar_cluster(sepsis_scaled)
cluster_similarity

cluster_similarity_default = similar_cluster_default(sepsis_scaled)
cluster_similarity_default

sepsis_scaled["DBSCAN_pred"] = sepsis_scaled["DBSCAN_pred"].astype(object)
sepsis_scaled["HDBSCAN_pred"] = sepsis_scaled["HDBSCAN_pred"].astype(object)
sepsis_scaled["Mean_shift_pred"] = sepsis_scaled["Mean_shift_pred"].astype(object)

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 9, "DBSCAN_pred"] = 'A'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 8, "HDBSCAN_pred"] = 'A'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 0, "Mean_shift_pred"] = 'A'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 11, "DBSCAN_pred"] = 'B'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 6, "HDBSCAN_pred"] = 'B'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 1, "Mean_shift_pred"] = 'B'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 15, "DBSCAN_pred"] = 'C'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 3, "HDBSCAN_pred"] = 'C'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 2, "Mean_shift_pred"] = 'C'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 14, "DBSCAN_pred"] = 'D'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 7, "HDBSCAN_pred"] = 'D'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 3, "Mean_shift_pred"] = 'D'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 16, "DBSCAN_pred"] = 'E'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 1, "HDBSCAN_pred"] = 'E'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 6, "Mean_shift_pred"] = 'E'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 19, "DBSCAN_pred"] = 'F'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 5, "HDBSCAN_pred"] = 'F'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 4, "Mean_shift_pred"] = 'F'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 21, "DBSCAN_pred"] = 'G'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 2, "HDBSCAN_pred"] = 'G'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 5, "Mean_shift_pred"] = 'G'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_pred"] == 8, "DBSCAN_pred"] = 'H'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_pred"] == 0, "HDBSCAN_pred"] = 'H'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_pred"] == 7, "Mean_shift_pred"] = 'H'


sepsis_scaled.loc[sepsis_scaled["DBSCAN_default_pred"] == 2, "DBSCAN_default_pred"] = 'A'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_default_pred"] == 6, "HDBSCAN_default_pred"] = 'A'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_default_pred"] == 0, "Mean_shift_default_pred"] = 'A'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_default_pred"] == 7, "DBSCAN_default_pred"] = 'B'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_default_pred"] == 11, "HDBSCAN_default_pred"] = 'B'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_default_pred"] == 1, "Mean_shift_default_pred"] = 'B'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_default_pred"] == -1, "DBSCAN_default_pred"] = 'C'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_default_pred"] == -1, "HDBSCAN_default_pred"] = 'C'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_default_pred"] == 2, "Mean_shift_default_pred"] = 'C'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_default_pred"] == 9, "DBSCAN_default_pred"] = 'D'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_default_pred"] == 1, "HDBSCAN_default_pred"] = 'D'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_default_pred"] == 4, "Mean_shift_default_pred"] = 'D'

sepsis_scaled.loc[sepsis_scaled["DBSCAN_default_pred"] == 12, "DBSCAN_default_pred"] = 'E'
sepsis_scaled.loc[sepsis_scaled["HDBSCAN_default_pred"] == 0, "HDBSCAN_default_pred"] = 'E'
sepsis_scaled.loc[sepsis_scaled["Mean_shift_default_pred"] == 3, "Mean_shift_default_pred"] = 'E'



sepsis_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]] = (
    sepsis_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]]
    .replace({'A': 0, 'B': 1, 'C': 2,'D': 3,'E': 4,'F': 5, 'G': 6, 'H': 7})
    .astype(int)
)

sepsis_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]] = (
    sepsis_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]]
    .replace({'A': 0, 'B': 1, 'C': 2, 'D':3, 'E': 4})
    .astype(int)
)

same_value_count = (sepsis_scaled["HDBSCAN_pred"] == sepsis_scaled["DBSCAN_pred"]) & \
                   (sepsis_scaled["DBSCAN_pred"] == sepsis_scaled["Mean_shift_pred"])

same_value_count_default = (sepsis_scaled["HDBSCAN_default_pred"] == sepsis_scaled["DBSCAN_default_pred"]) & \
                   (sepsis_scaled["DBSCAN_default_pred"] == sepsis_scaled["Mean_shift_default_pred"])

total_same_value = same_value_count.sum()
total_same_value_default = same_value_count_default.sum()
total_rows = len(sepsis_scaled)

percentage_same_value = (total_same_value / total_rows) * 100
print("Totale delle righe con lo stesso valore:", total_same_value)
print("Percentuale rispetto al totale:", round(percentage_same_value,2), "%")
percentage_same_value = round((percentage_same_value / 100),3)
ari = compute_ari(sepsis_scaled)

percentage_same_value_default = (total_same_value_default / total_rows) * 100
print("Totale delle righe con lo stesso valore con parametri default:", total_same_value_default)
print("Percentuale rispetto al totale:", round(percentage_same_value_default,2), "%")
percentage_same_value_default = round((percentage_same_value_default / 100),3)
ari_default = compute_ari_default(sepsis_scaled)

df_metrics_sepsis['Clustering Truth'] = df_metrics_sepsis['Method'].apply(
    lambda x: percentage_same_value_default if 'default' in x else percentage_same_value)

df_metrics_sepsis['ARI'] = df_metrics_sepsis['Method'].apply(
    lambda x: ari_default if 'default' in x else ari)

df_metrics_sepsis = df_metrics_sepsis.sort_values('DBCV', ascending=False)

df_metrics_sepsis

results = []
methods = ["DBSCAN", "HDBSCAN", "Meanshift"]
default_methods = {
    "DBSCAN": "DBSCAN_default",
    "HDBSCAN": "HDBSCAN_default",
    "Meanshift": "Meanshift_default"
}

metrics_distance = [
    "DBCV", "DBDBI",	"DBDI",	"DBCHI",	"CVDD"
]


for method in methods:
    for metric_value in metrics_distance:
        if metric_value in df_metrics_sepsis.columns:
            best_value = df_metrics_sepsis.loc[df_metrics_sepsis["Method"] == method, metric_value].values[0]
            default_value = df_metrics_sepsis.loc[df_metrics_sepsis["Method"] == default_methods[method], metric_value].values[0]
            trend, consistency = calculate_trend_and_consistency(df_metrics_sepsis,metric_value, method, default_methods[method])
            results.append({
                "Metric": metric_value.split(" ")[0],
                "Random_parameters": round(best_value,3),
                "Default": round(default_value,3),
                "Method": method,
                "Random_ARI": round(ari,3),
                "Default_ARI": round(ari_default,3),
                "trend": trend,
                "Trend consistency with ARI": consistency
            })

final_table = pd.DataFrame(results)
print("Tab.7 Trend Conisentecy With ARI :\n")
final_table

consistency_table = final_table.groupby("Metric")["Trend consistency with ARI"].value_counts().unstack(fill_value=0)
consistency_table = consistency_table.rename(columns={"Consistent": "N°Consistent", "Inconsistent": "N°Inconsistent"}).reset_index()
consistency_table = consistency_table.sort_values(by="N°Consistent", ascending=False).reset_index(drop=True)
#consistency_table = consistency_table.to_string(index=False)
final_summary_table = final_summary_table.merge(consistency_table, on="Metric", how="outer", suffixes=('', '_new')).fillna(0)
final_summary_table["N°Consistent"] += final_summary_table.pop("N°Consistent_new")
final_summary_table["N°Inconsistent"] += final_summary_table.pop("N°Inconsistent_new")
final_summary_table["Not Applicable"] += final_summary_table.pop("Not Applicable_new")
print(consistency_table)

"""#**Heart failure**"""

heart_failure = heart_failure.dropna(thresh=heart_failure.shape[1] - 2)
numerical_features = heart_failure.columns
scaler_minmax = MinMaxScaler()
heart_failure_scaled = heart_failure.copy()
heart_failure_scaled[numerical_features] = scaler_minmax.fit_transform(heart_failure_scaled[numerical_features])
heart_failure_scaled.reset_index(drop=True, inplace=True)
heart_failure_scaled.drop_duplicates(inplace=True)
df_metrics_heart_failure = pd.DataFrame()
heart_failure_scaled

"""**HDBSCAN**"""

if isinstance(heart_failure_scaled, pd.DataFrame):
    X_array = heart_failure_scaled.values
else:
    X_array = heart_failure_scaled

random.seed(44)
min_size = random.randint(5, 20)
epsilon = random.uniform(0.01, 1.0)
print(f"min_size: {min_size}, epsilon: {epsilon}" )

hdbscan_cluster= hdbscan.HDBSCAN(min_cluster_size=min_size, cluster_selection_epsilon=epsilon)
hdbscan_cluster_default= hdbscan.HDBSCAN()

labels = hdbscan_cluster.fit_predict(heart_failure_scaled)
labels_default = hdbscan_cluster_default.fit_predict(heart_failure_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv  = compute_metrics(X_array, labels)
heart_failure_scaled['HDBSCAN_pred'] = labels

methods = 'HDBSCAN'
methods_default = 'HDBSCAN_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default   = compute_metrics(X_array, labels_default)
heart_failure_scaled['HDBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_heart_failure = pd.concat([df_metrics_heart_failure, metrics_df], ignore_index=True)
df_metrics_heart_failure = pd.concat([df_metrics_heart_failure, metrics_df_default], ignore_index=True)
df_metrics_heart_failure

"""**DBSCAN**"""

random.seed(44)
min_sample = random.randint(2, 20)
eps = random.uniform(0.1, 2.0)
print(f"min_sample: {min_sample}, eps: {eps}" )

dbscan = DBSCAN(min_samples=min_sample, eps=eps)
dbscan_default = DBSCAN()

labels = dbscan.fit_predict(heart_failure_scaled)
labels_default = dbscan_default.fit_predict(heart_failure_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(X_array, labels)
heart_failure_scaled['DBSCAN_pred'] = labels

methods = 'DBSCAN'
methods_default = 'DBSCAN_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default  = compute_metrics(X_array, labels_default)
heart_failure_scaled['DBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}


metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_heart_failure = pd.concat([df_metrics_heart_failure, metrics_df], ignore_index=True)
df_metrics_heart_failure = pd.concat([df_metrics_heart_failure, metrics_df_default], ignore_index=True)
df_metrics_heart_failure

"""**Mean_Shift**"""

random.seed(44)
band = random.uniform(0.1, 5.0)
print(f"band: {band}" )

mean_shift = MeanShift(bandwidth=band)
mean_shift_default = MeanShift()

labels = mean_shift.fit_predict(heart_failure_scaled)
labels_default = mean_shift_default.fit_predict(heart_failure_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv = compute_metrics(X_array, labels)
heart_failure_scaled['Mean_shift_pred'] = labels

methods = 'Meanshift'
methods_default = 'Meanshift_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default = compute_metrics(X_array, labels_default)
heart_failure_scaled['Mean_shift_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_heart_failure = pd.concat([df_metrics_heart_failure, metrics_df], ignore_index=True)
df_metrics_heart_failure = pd.concat([df_metrics_heart_failure, metrics_df_default], ignore_index=True)
df_metrics_heart_failure

"""**Cluster Analysis**"""

cluster_similarity = similar_cluster(heart_failure_scaled)
cluster_similarity

cluster_similarity_default = similar_cluster_default(heart_failure_scaled)
cluster_similarity_default

heart_failure_scaled["DBSCAN_pred"] = heart_failure_scaled["DBSCAN_pred"].astype(object)
heart_failure_scaled["HDBSCAN_pred"] = heart_failure_scaled["HDBSCAN_pred"].astype(object)
heart_failure_scaled["Mean_shift_pred"] = heart_failure_scaled["Mean_shift_pred"].astype(object)

heart_failure_scaled.loc[heart_failure_scaled["DBSCAN_pred"] == -1, "DBSCAN_pred"] = 'A'
heart_failure_scaled.loc[heart_failure_scaled["HDBSCAN_pred"] == -1, "HDBSCAN_pred"] = 'A'
heart_failure_scaled.loc[heart_failure_scaled["Mean_shift_pred"] == 0, "Mean_shift_pred"] = 'A'

heart_failure_scaled.loc[heart_failure_scaled["DBSCAN_pred"] == 0, "DBSCAN_pred"] = 'B'
heart_failure_scaled.loc[heart_failure_scaled["HDBSCAN_pred"] == 3, "HDBSCAN_pred"] = 'B'
heart_failure_scaled.loc[heart_failure_scaled["Mean_shift_pred"] == 1, "Mean_shift_pred"] = 'B'

heart_failure_scaled.loc[heart_failure_scaled["DBSCAN_pred"] == 3, "DBSCAN_pred"] = 'C'
heart_failure_scaled.loc[heart_failure_scaled["HDBSCAN_pred"] == 2, "HDBSCAN_pred"] = 'C'
heart_failure_scaled.loc[heart_failure_scaled["Mean_shift_pred"] == 2, "Mean_shift_pred"] = 'C'

heart_failure_scaled.loc[heart_failure_scaled["DBSCAN_pred"] == 8, "DBSCAN_pred"] = 'D'
heart_failure_scaled.loc[heart_failure_scaled["HDBSCAN_pred"] == 0, "HDBSCAN_pred"] = 'D'
heart_failure_scaled.loc[heart_failure_scaled["Mean_shift_pred"] == 3, "Mean_shift_pred"] = 'D'


heart_failure_scaled.loc[heart_failure_scaled["DBSCAN_default_pred"] == -1, "DBSCAN_default_pred"] = 'A'
heart_failure_scaled.loc[heart_failure_scaled["HDBSCAN_default_pred"] == -1, "HDBSCAN_default_pred"] = 'A'
heart_failure_scaled.loc[heart_failure_scaled["Mean_shift_default_pred"] == 0, "Mean_shift_default_pred"] = 'A'

heart_failure_scaled.loc[heart_failure_scaled["DBSCAN_default_pred"] == 4, "DBSCAN_default_pred"] = 'B'
heart_failure_scaled.loc[heart_failure_scaled["HDBSCAN_default_pred"] == 15, "HDBSCAN_default_pred"] = 'B'
heart_failure_scaled.loc[heart_failure_scaled["Mean_shift_default_pred"] == 1, "Mean_shift_default_pred"] = 'B'

heart_failure_scaled.loc[heart_failure_scaled["DBSCAN_default_pred"] == 15, "DBSCAN_default_pred"] = 'C'
heart_failure_scaled.loc[heart_failure_scaled["HDBSCAN_default_pred"] == 10, "HDBSCAN_default_pred"] = 'C'
heart_failure_scaled.loc[heart_failure_scaled["Mean_shift_default_pred"] == 2, "Mean_shift_default_pred"] = 'C'

heart_failure_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]] = (
    heart_failure_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]]
    .replace({'A': 0, 'B': 1, 'C': 2,'D': 3})
    .astype(int)
)

heart_failure_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]] = (
    heart_failure_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]]
    .replace({'A': 0, 'B': 1, 'C':2})
    .astype(int)
)

same_value_count = (heart_failure_scaled["HDBSCAN_pred"] == heart_failure_scaled["DBSCAN_pred"]) & \
                   (heart_failure_scaled["DBSCAN_pred"] == heart_failure_scaled["Mean_shift_pred"])

same_value_count_default = (heart_failure_scaled["HDBSCAN_default_pred"] == heart_failure_scaled["DBSCAN_default_pred"]) & \
                   (heart_failure_scaled["DBSCAN_default_pred"] == heart_failure_scaled["Mean_shift_default_pred"])

total_same_value = same_value_count.sum()
total_same_value_default = same_value_count_default.sum()
total_rows = len(heart_failure_scaled)

percentage_same_value = (total_same_value / total_rows) * 100
print("Totale delle righe con lo stesso valore:", total_same_value)
print("Percentuale rispetto al totale:", round(percentage_same_value,2), "%")
percentage_same_value = round((percentage_same_value / 100),3)
ari = compute_ari(heart_failure_scaled)

percentage_same_value_default = (total_same_value_default / total_rows) * 100
print("Totale delle righe con lo stesso valore con parametri default:", total_same_value_default)
print("Percentuale rispetto al totale:", round(percentage_same_value_default,2), "%")
percentage_same_value_default = round((percentage_same_value_default / 100),3)
ari_default = compute_ari_default(heart_failure_scaled)

df_metrics_heart_failure['Clustering Truth'] = df_metrics_heart_failure['Method'].apply(
    lambda x: percentage_same_value_default if 'default' in x else percentage_same_value)

df_metrics_heart_failure['ARI'] = df_metrics_heart_failure['Method'].apply(
    lambda x: ari_default if 'default' in x else ari)

df_metrics_heart_failure = df_metrics_heart_failure.sort_values('DBCV', ascending=False)

results = []
methods = ["DBSCAN", "HDBSCAN", "Meanshift"]
default_methods = {
    "DBSCAN": "DBSCAN_default",
    "HDBSCAN": "HDBSCAN_default",
    "Meanshift": "Meanshift_default"
}
metrics_distance = [
    "DBCV", "DBDBI",	"DBDI",	"DBCHI",	"CVDD"
]


for method in methods:
    for metric_value in metrics_distance:
        if metric_value in df_metrics_heart_failure.columns:
            best_value = df_metrics_heart_failure.loc[df_metrics_heart_failure["Method"] == method, metric_value].values[0]
            default_value = df_metrics_heart_failure.loc[df_metrics_heart_failure["Method"] == default_methods[method], metric_value].values[0]
            trend, consistency = calculate_trend_and_consistency(df_metrics_heart_failure,metric_value, method, default_methods[method])
            results.append({
                "Metric": metric_value.split(" ")[0],
                "Random_parameters": round(best_value,3),
                "Default": round(default_value,3),
                "Method": method,
                "Random_ARI": round(ari,3),
                "Default_ARI": round(ari_default,3),
                "trend": trend,
                "Trend consistency with ARI": consistency
            })

final_table = pd.DataFrame(results)
print("Tab.9 Trend Conisentecy With ARI :\n")
final_table

consistency_table = final_table.groupby("Metric")["Trend consistency with ARI"].value_counts().unstack(fill_value=0)
consistency_table = consistency_table.rename(columns={"Consistent": "N°Consistent", "Inconsistent": "N°Inconsistent"}).reset_index()
consistency_table = consistency_table.sort_values(by="N°Consistent", ascending=False).reset_index(drop=True)
#consistency_table = consistency_table.to_string(index=False)
final_summary_table = final_summary_table.merge(consistency_table, on="Metric", how="outer", suffixes=('', '_new')).fillna(0)
final_summary_table["N°Consistent"] += final_summary_table.pop("N°Consistent_new")
final_summary_table["N°Inconsistent"] += final_summary_table.pop("N°Inconsistent_new")
final_summary_table["Not Applicable"] += final_summary_table.pop("Not Applicable_new")
consistency_table

"""#**Cardiac Arrest**"""

cardiac_arrest = cardiac_arrest.fillna(0)
numerical_features = cardiac_arrest.columns
scaler_minmax = MinMaxScaler()
cardiac_arrest_scaled = cardiac_arrest.copy()
cardiac_arrest_scaled[numerical_features] = scaler_minmax.fit_transform(cardiac_arrest_scaled[numerical_features])
cardiac_arrest_scaled.reset_index(drop=True, inplace=True)
cardiac_arrest_scaled.drop_duplicates(inplace=True)
df_metrics_cardiac_arrest = pd.DataFrame()
cardiac_arrest_scaled

"""**HDBSCAN**"""

if isinstance(cardiac_arrest_scaled, pd.DataFrame):
    X_array = cardiac_arrest_scaled.values
else:
    X_array = cardiac_arrest_scaled

random.seed(4)
min_size = random.randint(5, 20)
epsilon = random.uniform(0.01, 1.0)
print(f"min_size: {min_size}, epsilon: {epsilon}" )

hdbscan_cluster= hdbscan.HDBSCAN(min_cluster_size=min_size, cluster_selection_epsilon=epsilon)
hdbscan_cluster_default= hdbscan.HDBSCAN()

labels = hdbscan_cluster.fit_predict(cardiac_arrest_scaled)
labels_default = hdbscan_cluster_default.fit_predict(cardiac_arrest_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv  = compute_metrics(X_array, labels)
cardiac_arrest_scaled['HDBSCAN_pred'] = labels

methods = 'HDBSCAN'
methods_default = 'HDBSCAN_default'

metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}
dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default  = compute_metrics(X_array, labels_default)
cardiac_arrest_scaled['HDBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])

df_metrics_cardiac_arrest = pd.concat([df_metrics_cardiac_arrest, metrics_df], ignore_index=True)
df_metrics_cardiac_arrest = pd.concat([df_metrics_cardiac_arrest, metrics_df_default], ignore_index=True)
df_metrics_cardiac_arrest

"""**DBSCAN**"""

random.seed(4)
min_sample = random.randint(2, 20)
eps = random.uniform(0.1, 2.0)
print(f"min_sample: {min_sample}, eps: {eps}" )

dbscan = DBSCAN(min_samples=min_sample, eps=eps)
dbscan_default = DBSCAN()

labels = dbscan.fit_predict(cardiac_arrest_scaled)
labels_default = dbscan_default.fit_predict(cardiac_arrest_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv  = compute_metrics(X_array, labels)
cardiac_arrest_scaled['DBSCAN_pred'] = labels

methods = 'DBSCAN'
methods_default = 'DBSCAN_default'
metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default  = compute_metrics(X_array, labels_default)
cardiac_arrest_scaled['DBSCAN_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}


metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_cardiac_arrest = pd.concat([df_metrics_cardiac_arrest, metrics_df], ignore_index=True)
df_metrics_cardiac_arrest = pd.concat([df_metrics_cardiac_arrest, metrics_df_default], ignore_index=True)
df_metrics_cardiac_arrest

"""**Mean_Shift**"""

random.seed(4)
band = random.uniform(0.1, 5.0)
print(f"band: {band}" )

mean_shift = MeanShift(bandwidth=band)
mean_shift_default = MeanShift()

labels = mean_shift.fit_predict(cardiac_arrest_scaled)
labels_default = mean_shift_default.fit_predict(cardiac_arrest_scaled)

dbcv_score, db_dbcv, du_dbcv, chi_dbcv, cvdd_dbcv  = compute_metrics(X_array, labels)
cardiac_arrest_scaled['Mean_shift_pred'] = labels

methods = 'Meanshift'
methods_default = 'Meanshift_default'
metrics_dict = {
    'Method': methods,
    'DBCV': dbcv_score,
    'DBDBI': db_dbcv,
    'DBDI': du_dbcv,
    'DBCHI': chi_dbcv,
    'CVDD': cvdd_dbcv
}

dbcv_score_default, db_dbcv_default, du_dbcv_default, chi_dbcv_default, cvdd_dbcv_default   = compute_metrics(X_array, labels_default)
cardiac_arrest_scaled['Mean_shift_default_pred'] = labels_default

metrics_dict_default = {
    'Method': methods_default,
    'DBCV': dbcv_score_default,
    'DBDBI': db_dbcv_default,
    'DBDI': du_dbcv_default,
    'DBCHI': chi_dbcv_default,
    'CVDD': cvdd_dbcv_default

}


metrics_df = pd.DataFrame([metrics_dict])
metrics_df_default = pd.DataFrame([metrics_dict_default])
df_metrics_cardiac_arrest = pd.concat([df_metrics_cardiac_arrest, metrics_df], ignore_index=True)
df_metrics_cardiac_arrest = pd.concat([df_metrics_cardiac_arrest, metrics_df_default], ignore_index=True)
df_metrics_cardiac_arrest

"""**Cluster Analysis**"""

cluster_similarity = similar_cluster(cardiac_arrest_scaled)
cluster_similarity

cluster_similarity_default = similar_cluster_default(cardiac_arrest_scaled)
cluster_similarity_default

cardiac_arrest_scaled["DBSCAN_pred"] = cardiac_arrest_scaled["DBSCAN_pred"].astype(object)
cardiac_arrest_scaled["HDBSCAN_pred"] = cardiac_arrest_scaled["HDBSCAN_pred"].astype(object)
cardiac_arrest_scaled["Mean_shift_pred"] = cardiac_arrest_scaled["Mean_shift_pred"].astype(object)

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == -1, "DBSCAN_pred"] = 'A'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == -1, "HDBSCAN_pred"] = 'A'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 0, "Mean_shift_pred"] = 'A'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 0, "DBSCAN_pred"] = 'B'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 8, "HDBSCAN_pred"] = 'B'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 1, "Mean_shift_pred"] = 'B'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 9, "DBSCAN_pred"] = 'C'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 7, "HDBSCAN_pred"] = 'C'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 2, "Mean_shift_pred"] = 'C'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 5, "DBSCAN_pred"] = 'D'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 4, "HDBSCAN_pred"] = 'D'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 3, "Mean_shift_pred"] = 'D'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 2, "DBSCAN_pred"] = 'E'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 2, "HDBSCAN_pred"] = 'E'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 5, "Mean_shift_pred"] = 'E'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 6, "DBSCAN_pred"] = 'F'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 3, "HDBSCAN_pred"] = 'F'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 6, "Mean_shift_pred"] = 'F'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 8, "DBSCAN_pred"] = 'G'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 0, "HDBSCAN_pred"] = 'G'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 7, "Mean_shift_pred"] = 'G'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 1, "DBSCAN_pred"] = 'H'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 6, "HDBSCAN_pred"] = 'H'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 8, "Mean_shift_pred"] = 'H'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_pred"] == 10, "DBSCAN_pred"] = 'I'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_pred"] == 1, "HDBSCAN_pred"] = 'I'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_pred"] == 9, "Mean_shift_pred"] = 'I'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_default_pred"] == -1, "DBSCAN_default_pred"] = 'A'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_default_pred"] == -1, "HDBSCAN_default_pred"] = 'A'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_default_pred"] == 2, "Mean_shift_default_pred"] = 'A'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_default_pred"] == 0, "DBSCAN_default_pred"] = 'B'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_default_pred"] == 23, "HDBSCAN_default_pred"] = 'B'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_default_pred"] == 0, "Mean_shift_default_pred"] = 'B'

cardiac_arrest_scaled.loc[cardiac_arrest_scaled["DBSCAN_default_pred"] == 12, "DBSCAN_default_pred"] = 'C'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["HDBSCAN_default_pred"] == 10, "HDBSCAN_default_pred"] = 'C'
cardiac_arrest_scaled.loc[cardiac_arrest_scaled["Mean_shift_default_pred"] == 1, "Mean_shift_default_pred"] = 'C'

cardiac_arrest_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]] = (
    cardiac_arrest_scaled[["DBSCAN_pred", "HDBSCAN_pred", "Mean_shift_pred"]]
    .replace({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8,})
    .astype(int)
)

cardiac_arrest_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]] = (
    cardiac_arrest_scaled[["DBSCAN_default_pred", "HDBSCAN_default_pred", "Mean_shift_default_pred"]]
    .replace({'A': 0, 'B': 1, 'C':2})
    .astype(int)
)

same_value_count = (cardiac_arrest_scaled["HDBSCAN_pred"] == cardiac_arrest_scaled["DBSCAN_pred"]) & \
                   (cardiac_arrest_scaled["DBSCAN_pred"] == cardiac_arrest_scaled["Mean_shift_pred"])

same_value_count_default = (cardiac_arrest_scaled["HDBSCAN_default_pred"] == cardiac_arrest_scaled["DBSCAN_default_pred"]) & \
                   (cardiac_arrest_scaled["DBSCAN_default_pred"] == cardiac_arrest_scaled["Mean_shift_default_pred"])

total_same_value = same_value_count.sum()
total_same_value_deafult = same_value_count_default.sum()
total_rows = len(cardiac_arrest_scaled)

percentage_same_value = (total_same_value / total_rows) * 100
print("Totale delle righe con lo stesso valore:", total_same_value)
print("Percentuale rispetto al totale:", round(percentage_same_value,2), "%")
percentage_same_value = round((percentage_same_value / 100),3)
ari = compute_ari(cardiac_arrest_scaled)

percentage_same_value_default = (total_same_value_default / total_rows) * 100
print("Totale delle righe con lo stesso valore con parametri default:", total_same_value_default)
print("Percentuale rispetto al totale:", round(percentage_same_value_default,2), "%")
percentage_same_value_default = round((percentage_same_value_default / 100),3)
ari_default = compute_ari_default(cardiac_arrest_scaled)

df_metrics_cardiac_arrest['Clustering Truth'] = df_metrics_cardiac_arrest['Method'].apply(
    lambda x: percentage_same_value_default if 'default' in x else percentage_same_value)

df_metrics_cardiac_arrest['ARI'] = df_metrics_cardiac_arrest['Method'].apply(
    lambda x: ari_default if 'default' in x else ari)

df_metrics_cardiac_arrest = df_metrics_cardiac_arrest.sort_values('DBCV', ascending=False)

df_metrics_cardiac_arrest

results = []
methods = ["DBSCAN", "HDBSCAN", "Meanshift"]
default_methods = {
    "DBSCAN": "DBSCAN_default",
    "HDBSCAN": "HDBSCAN_default",
    "Meanshift": "Meanshift_default"
}
metrics_distance = [
    "DBCV", "DBDBI",	"DBDI",	"DBCHI",	"CVDD"
]


for method in methods:
    for metric_value in metrics_distance:
        if metric_value in df_metrics_cardiac_arrest.columns:
            best_value = df_metrics_cardiac_arrest.loc[df_metrics_cardiac_arrest["Method"] == method, metric_value].values[0]
            default_value = df_metrics_cardiac_arrest.loc[df_metrics_cardiac_arrest["Method"] == default_methods[method], metric_value].values[0]
            trend, consistency = calculate_trend_and_consistency(df_metrics_cardiac_arrest,metric_value, method, default_methods[method])
            results.append({
                "Metric": metric_value.split(" ")[0],
                "Random_parameters": round(best_value,3),
                "Default": round(default_value,3),
                "Method": method,
                "Random_ARI": round(ari,3),
                "Default_ARI": round(ari_default,3),
                "trend": trend,
                "Trend consistency with ARI": consistency
            })

final_table = pd.DataFrame(results)
print("Tab.11 Trend Conisentecy With ARI :\n")
final_table

consistency_table = final_table.groupby("Metric")["Trend consistency with ARI"].value_counts().unstack(fill_value=0)
consistency_table = consistency_table.rename(columns={"Consistent": "N°Consistent", "Inconsistent": "N°Inconsistent"}).reset_index()
consistency_table = consistency_table.sort_values(by="N°Consistent", ascending=False).reset_index(drop=True)
#consistency_table = consistency_table.to_string(index=False)
final_summary_table = final_summary_table.merge(consistency_table, on="Metric", how="outer", suffixes=('', '_new')).fillna(0)
final_summary_table["N°Consistent"] += final_summary_table.pop("N°Consistent_new")
final_summary_table["N°Inconsistent"] += final_summary_table.pop("N°Inconsistent_new")
final_summary_table["Not Applicable"] += final_summary_table.pop("Not Applicable_new")
consistency_table

"""#**Final Consistency Table**"""

final_summary_table = final_summary_table.sort_values(by="N°Consistent", ascending=False).reset_index(drop=True)
final_styled_table = final_summary_table.style.hide(axis="index")
final_styled_table

final_summary_table["% Consistent"] = (final_summary_table["N°Consistent"] / 15) * 100
final_summary_table["% Consistent"] = final_summary_table["% Consistent"].fillna(0)
plt.figure(figsize=(8, 5))
bars = plt.bar(final_summary_table["Metric"], final_summary_table["% Consistent"], color="#69b3a2")

import matplotlib.pyplot as plt

colors = ["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728"]

fig, ax = plt.subplots(figsize=(6, 6))  # leggermente più largo

bars = ax.bar(final_summary_table["Metric"], final_summary_table["% Consistent"], color=colors)

# Aggiunta percentuali sopra le barre
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2.0, height + 1, f"{height:.0f}%",
            ha='center', va='bottom', fontsize=10)

# Miglioramenti estetici
ax.set_axisbelow(True)
ax.yaxis.grid(True, linestyle='--', alpha=0.7)
ax.xaxis.grid(False)

ax.set_ylabel("% tests producing consistent trend", fontsize=12)
ax.set_xlabel("Metric", fontsize=12)
ax.set_ylim(0, 100)
ax.tick_params(axis='both', labelsize=11)

# Rotazione delle etichette sull’asse X
ax.set_xticklabels(final_summary_table["Metric"], rotation=25, ha='right')

# Legenda
handles = [plt.Rectangle((0,0),1,1, color=color) for color in colors]
legend_labels = final_summary_table["Metric"]
ax.legend(handles, legend_labels, title="Metric", loc="upper right", fontsize=10, title_fontsize=11)

plt.tight_layout()
plt.savefig("consistencies.pdf", format="pdf")